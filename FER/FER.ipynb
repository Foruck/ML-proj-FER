{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML final face.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmV2wQoo2TT6",
        "colab_type": "text"
      },
      "source": [
        "# prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "immvDoMWM614",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "import os\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
        "\n",
        "if not os.path.exists(\"drive\"):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')\n",
        "    !ln -s /gdrive/My\\ Drive/ML drive_ml\n",
        "else:\n",
        "    !ln -s /gdrive/My\\ Drive/ML drive_ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA_A0QtrZ0Oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import time, os, json, logging, random, h5py, shutil\n",
        "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# config\n",
        "exp_name = 'inception224'\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "num_epochs = 100\n",
        "decay_epoch = [40, 70, 90]\n",
        "\n",
        "crop_size = 44\n",
        "input_ch  = 1\n",
        "batch_size = 32\n",
        "\n",
        "valid_epoch = 5\n",
        "\n",
        "output_ch = 7\n",
        "num_workers = 8\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i34uY64nHFXU",
        "colab_type": "text"
      },
      "source": [
        "## prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvpYKEzk1IuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "raw_data = {}\n",
        "with h5py.File('/content/drive_ml/data.h5', 'r') as fp:\n",
        "    for ds in ['train', 'valid', 'test']:\n",
        "        raw_data[ds] = {}\n",
        "        raw_data[ds]['data'] = np.array(fp[ds]['data']).reshape((-1, 48,48)).astype(np.uint8)\n",
        "        raw_data[ds]['label'] = np.array(fp[ds]['label'])\n",
        "\n",
        "class FerDataset(Dataset):\n",
        "    def __init__(self, input_ch, data_label, transform=None):\n",
        "        self.data = data_label['data']\n",
        "        self.label= torch.LongTensor(data_label['label'])\n",
        "        self.transform = transform\n",
        "        self.input_ch = input_ch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dt = self.data[idx,:]\n",
        "        dt = np.stack([dt]*self.input_ch, axis=2)\n",
        "        dt = self.transform(dt)\n",
        "        return dt, self.label[idx]\n",
        "    \n",
        "def create_dataloader(crop_size, input_size):\n",
        "    if crop_size==48:\n",
        "        transform = {\n",
        "            'train':None, \n",
        "            'valid':None,\n",
        "            'test': None\n",
        "        }\n",
        "    else:\n",
        "        transform = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.ToPILImage(mode='L'),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomCrop(crop_size),\n",
        "                transforms.Resize(input_size),\n",
        "                transforms.ToTensor()\n",
        "            ]),\n",
        "            'valid':transforms.Compose([\n",
        "                transforms.ToPILImage(mode='L'),\n",
        "                transforms.CenterCrop(crop_size),\n",
        "                transforms.Resize(input_size),\n",
        "                transforms.ToTensor(),\n",
        "            ]),\n",
        "            'test': transforms.Compose([\n",
        "                transforms.ToPILImage(mode='L'),\n",
        "                transforms.CenterCrop(crop_size),\n",
        "                transforms.Resize(input_size),\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "        }\n",
        "\n",
        "\n",
        "    datasets = { x: FerDataset(input_ch, raw_data[x], transform[x]) for x in ['train', 'valid', 'test'] }\n",
        "\n",
        "    logging.info(', '.join([\n",
        "        \"{} {}\".format(x, len(datasets[x]))\n",
        "        for x in ['train', 'valid', 'test']\n",
        "    ]))\n",
        "\n",
        "    dataloaders = {\n",
        "        \"train\": DataLoader(\n",
        "            datasets['train'], \n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=num_workers\n",
        "        ), \n",
        "        \"valid\": DataLoader(\n",
        "            datasets['valid'], \n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers\n",
        "        ), \n",
        "        \"test\": DataLoader(\n",
        "            datasets['test'], \n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers\n",
        "        ),\n",
        "    }\n",
        "    return datasets, dataloaders\n",
        "\n",
        "#datasets, dataloaders = create_dataloader(crop_size, crop_size)\n",
        "#datasets_incep, dataloaders_incep = create_dataloader(crop_size, 299)\n",
        "#datasets_dense, dataloaders_dense = create_dataloader(crop_size, 224)\n",
        "\n",
        "datasets, dataloaders = create_dataloader(crop_size, 224)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwApqpe1vcYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet18(nn.Module):\n",
        "    name = 'ResNet18'\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(ResNet18, self).__init__()\n",
        "        \n",
        "        self.resnet = models.resnet18(pretrained=False)\n",
        "            \n",
        "        if input_ch!=3:\n",
        "            self.resnet.conv1 = torch.nn.Conv2d(input_ch, 64, kernel_size=7)\n",
        "            \n",
        "        old_n_ch = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(old_n_ch, output_ch)\n",
        "        \n",
        "        \n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.resnet(x)\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    \n",
        "class ResNet50(nn.Module):\n",
        "    name = 'ResNet50'\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(ResNet50, self).__init__()\n",
        "        \n",
        "        self.resnet = models.resnet50(pretrained=False)\n",
        "            \n",
        "        if input_ch!=3:\n",
        "            self.resnet.conv1 = torch.nn.Conv2d(input_ch, 64, kernel_size=7)\n",
        "            \n",
        "        old_n_ch = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(old_n_ch, output_ch)\n",
        "        \n",
        "        \n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.resnet(x)\n",
        "        return out\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "class VGG11(nn.Module):\n",
        "    name = 'VGG11'\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(VGG11, self).__init__()\n",
        "        \n",
        "        self.vgg11 = models.vgg11(pretrained=False)\n",
        "        \n",
        "        old_n_ch = self.vgg11.classifier[6].in_features\n",
        "        self.vgg11.classifier[6] = nn.Linear(old_n_ch, output_ch)\n",
        "        \n",
        "        for param in self.vgg11.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        \n",
        "        if input_ch!=3:\n",
        "            self.vgg11.features[0] = torch.nn.Conv2d(input_ch, 64, kernel_size=7)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.vgg11(x)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, input_ch, expand=True):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        output_ch = input_ch*2 if expand else input_ch\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(input_ch, output_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(output_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(output_ch, output_ch, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(output_ch)\n",
        "        )\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Conv2d(input_ch, output_ch, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(output_ch)\n",
        "        ) if expand else None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "\n",
        "        if self.downsample:\n",
        "            identity = self.downsample(x)\n",
        "        else:\n",
        "            identity = x\n",
        "\n",
        "        out = out+identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LightResNet(nn.Module):\n",
        "    \"\"\"9 layer, suitable for 44x44 image\"\"\"\n",
        "    \n",
        "    name = 'LightResNet'\n",
        "    \n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(LightResNet, self).__init__()\n",
        "        \n",
        "        # 第一层conv换成小一点的kernel\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(input_ch, 64, kernel_size=5, bias=False),   # 40x40x64ch\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # 第一层pooling不用了\n",
        "        \n",
        "        self.conv2_block = ResBlock(64, expand=False)   # 20x20x64ch\n",
        "        self.conv3_block = ResBlock(64)                 # 10x10x128ch\n",
        "        self.conv4_block = ResBlock(128)                # 5x5x256ch\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))     # 256ch\n",
        "        self.fc1 = nn.Linear(256, 7)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2_block(x)\n",
        "        x = self.conv3_block(x)\n",
        "        x = self.conv4_block(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    \n",
        "from torchvision.models import inception\n",
        "\n",
        "class Inception3(nn.Module):\n",
        "    name = 'Inception3'\n",
        "    \n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(Inception3, self).__init__()\n",
        "        \n",
        "        self.inception = inception.Inception3(num_classes=output_ch, aux_logits=False)\n",
        "        self.inception.Conv2d_1a_3x3 = inception.BasicConv2d(input_ch, 32, kernel_size=3, stride=2)\n",
        "        \n",
        "        for param in self.inception.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.inception(x)\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    \n",
        "class DenseNet121(nn.Module):\n",
        "    name = 'DenseNet121'\n",
        "    def __init__(self, input_ch, output_ch):\n",
        "        super(DenseNet121, self).__init__()\n",
        "        \n",
        "        self.densenet = models.densenet121(pretrained=False, progress=False)\n",
        "            \n",
        "        if input_ch!=3:\n",
        "            self.densenet.features[0] = nn.Conv2d(\n",
        "                input_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "            \n",
        "        old_n_ch = self.densenet.classifier.in_features\n",
        "        self.densenet.classifier = nn.Linear(old_n_ch, output_ch)\n",
        "        \n",
        "        for param in self.densenet.parameters():\n",
        "            param.requires_grad = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.densenet(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NbxUjm48VNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, logdir, resume_checkpoint=None, override=False):\n",
        "    def train_one_epoch(epoch):\n",
        "        model.train(True)\n",
        "\n",
        "        tot_loss = 0.0\n",
        "        tot_corr = 0\n",
        "        tot_iter = len(datasets['train'])\n",
        "\n",
        "        for inputs, labels in dataloaders['train']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            outputs = nn.Softmax(dim=1)(outputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            tot_loss += loss.item()\n",
        "            tot_corr += torch.sum(preds == labels).item()\n",
        "\n",
        "        logging.info('Train loss: {:.5f}, acc: {:.3f}%'.format(\n",
        "            tot_loss / tot_iter, \n",
        "            100 * tot_corr / tot_iter\n",
        "        ))\n",
        "    \n",
        "    def validate_one_epoch(epoch, best_val_acc):\n",
        "        model.eval()\n",
        "\n",
        "        tot_loss = 0.0\n",
        "        tot_corr = 0\n",
        "        tot_iter = len(datasets['valid'])\n",
        "\n",
        "        for inputs, labels in dataloaders['valid']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            outputs = nn.Softmax(dim=1)(outputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            tot_loss += loss.item()\n",
        "            tot_corr += torch.sum(preds == labels).item()\n",
        "        \n",
        "        epoch_loss = tot_loss / tot_iter\n",
        "        epoch_acc = tot_corr / tot_iter\n",
        "        logging.info('Valid Loss: {:.5f} Acc: {:.3f}%'.format(\n",
        "            epoch_loss,\n",
        "            100 * epoch_acc\n",
        "        ))\n",
        "        \n",
        "        \n",
        "        latest_path = os.path.join(logdir, 'latest.pth.tar')\n",
        "        best_path   = os.path.join(logdir, 'best.pth.tar')\n",
        "\n",
        "        torch.save({\n",
        "            'epoch'     : epoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer' : optimizer.state_dict(),\n",
        "            'scheduler' : scheduler.state_dict(),\n",
        "            'val_acc'   : epoch_acc\n",
        "        }, latest_path)\n",
        "        if epoch_acc > best_val_acc:\n",
        "            shutil.copyfile(latest_path, best_path)\n",
        "            \n",
        "        logging.info(\"Checkpoint saved\")\n",
        "        return epoch_acc\n",
        "    \n",
        "    \n",
        "    start_epoch = 0\n",
        "    best_val_acc = 0.0\n",
        "    chechpoint_epoch = None\n",
        "    \n",
        "    \n",
        "    if resume_checkpoint is not None:\n",
        "        checkpoint = torch.load(resume_checkpoint)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        best_val_acc = checkpoint['val_acc']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "    else:\n",
        "        assert override or not os.path.exists(logdir), logdir\n",
        "        os.makedirs(logdir, exist_ok=override)\n",
        "    \n",
        "    logging.info(\"Log directory: \"+logdir)\n",
        "    \n",
        "    \n",
        "    try:\n",
        "        for epoch in range(start_epoch, num_epochs):\n",
        "            logging.info('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "            scheduler.step()\n",
        "            train_one_epoch(epoch)\n",
        "            chechpoint_epoch = epoch\n",
        "            \n",
        "            if (epoch+1)%valid_epoch==0:\n",
        "                val_acc = validate_one_epoch(epoch, best_val_acc)\n",
        "                best_val_acc = max(best_val_acc, val_acc)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logging.warning(\"interrupted, latest chechpoint is at epoch {}\".format(epoch))\n",
        "\n",
        "    logging.info('Training complete with best val acc: {:4f}'.format(best_val_acc))\n",
        "\n",
        "    \n",
        "    \n",
        "def test_model(model, criterion, checkpoint_path, data_type='test'):\n",
        "    assert os.path.exists(checkpoint_path)\n",
        "    logging.info(\"Checkpoint: \"+checkpoint_path)\n",
        "    \n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tot_loss = 0.0\n",
        "    tot_corr = 0\n",
        "    tot_iter = len(datasets[data_type])\n",
        "\n",
        "    for inputs, labels in dataloaders[data_type]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        outputs = nn.Softmax(dim=1)(outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        tot_loss += loss.item()\n",
        "        tot_corr += torch.sum(preds == labels).item()\n",
        "\n",
        "    logging.info('On {} set: Loss: {:.5f} Acc: {:.3f}%'.format(\n",
        "        data_type,\n",
        "        tot_loss / tot_iter,\n",
        "        100 * tot_corr / tot_iter\n",
        "    ))\n",
        "    \n",
        "\n",
        "\n",
        "def model_list_inference(model_list, checkpoint_path_list):\n",
        "    for model, checkpoint_path in zip(model_list, checkpoint_path_list):\n",
        "        assert os.path.exists(checkpoint_path), checkpoint_path\n",
        "        logging.info(\"Checkpoint: \"+checkpoint_path)\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "        model.eval()\n",
        "    \n",
        "    inference_result = []\n",
        "\n",
        "    for i, ((inputs, labels), (inputs_in, _), (inputs_den, _)) in enumerate(zip(dataloaders['test'], dataloaders_incep['test'], dataloaders_dense['test'])):\n",
        "        inputs = inputs.to(device)\n",
        "        inputs_in = inputs_in.to(device)\n",
        "        inputs_den = inputs_den.to(device)\n",
        "        \n",
        "        outputs = []\n",
        "        for i, model in enumerate(model_list):\n",
        "            if model.name=='Inception3':\n",
        "                out = model(inputs_in)\n",
        "            elif model.name=='DenseNet121':\n",
        "                out = model(inputs_den)\n",
        "            else:\n",
        "                out = model(inputs)\n",
        "            outputs.append(out.cpu().detach())\n",
        "            \n",
        "        inference_result.append((outputs, labels))\n",
        "    return inference_result\n",
        "\n",
        "\n",
        "def test_fusion(inference_result, softmax=False, weight=None, verbose=True):\n",
        "    if weight is None:\n",
        "        weight = [1]*len(inference_result[0][0])\n",
        "    \n",
        "        \n",
        "    tot_corr = 0\n",
        "    tot_iter = len(datasets['test'])\n",
        "\n",
        "    for i, (outputs, labels)  in enumerate(inference_result):\n",
        "        preds = []\n",
        "        for j, x in enumerate(outputs):\n",
        "            if softmax:\n",
        "                x = nn.Softmax(dim=1)(x)\n",
        "            preds.append(x*weight[j])\n",
        "        \n",
        "        preds = torch.stack(preds, dim=0).sum(dim=0)\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        \n",
        "        tot_corr += torch.sum(preds == labels).item()\n",
        "    if verbose:\n",
        "        logging.info('Test Acc: {:.3f}%'.format(\n",
        "            100 * tot_corr / tot_iter\n",
        "        ))\n",
        "    \n",
        "    return tot_corr / tot_iter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC2vhFUXHIkT",
        "colab_type": "text"
      },
      "source": [
        "## train/test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOs-rU2uiRq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################################################\n",
        "#                                  Training                                        #\n",
        "####################################################################################\n",
        "\n",
        "model     = Inception3(input_ch, output_ch).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=decay_epoch, gamma=0.1)\n",
        "\n",
        "train_model(model, criterion, optimizer, scheduler, \n",
        "            num_epochs=num_epochs, \n",
        "            logdir='drive_ml/log/{}'.format(exp_name),\n",
        "            #resume_checkpoint='drive_ml/log/inception224/latest.pth.tar'\n",
        "            #override=True\n",
        "           )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX_mn_5UirIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################################################\n",
        "#                                  Testing                                         #\n",
        "####################################################################################\n",
        "\n",
        "\n",
        "test_model(model, criterion, checkpoint_path='drive_ml/log/{}/best.pth.tar'.format(exp_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGBDUQ3-Y2Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model     = Inception3(input_ch, output_ch).to(device)\n",
        "test_model(model, criterion, checkpoint_path='drive_ml/log/inception/best.pth.tar', data_type='train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgNU2wEKxSgX",
        "colab_type": "text"
      },
      "source": [
        "## fuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yYgqfe9xTxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inference_result = model_list_inference([\n",
        "    DenseNet121(input_ch, output_ch).to(device),\n",
        "    Inception3(input_ch, output_ch).to(device),\n",
        "    LightResNet(input_ch, output_ch).to(device),\n",
        "    ResNet50(input_ch, output_ch).to(device),\n",
        "    ResNet18(input_ch, output_ch).to(device),\n",
        "    VGG11(input_ch, output_ch).to(device),\n",
        "],\n",
        "[\n",
        "    'drive_ml/log/densenet/best.pth.tar',\n",
        "    'drive_ml/log/inception/best.pth.tar',\n",
        "    'drive_ml/log/lightres/best.pth.tar',\n",
        "    'drive_ml/log/resnet50/best.pth.tar',\n",
        "    'drive_ml/log/resnet18/best.pth.tar',\n",
        "    'drive_ml/log/vgg11/best.pth.tar',\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCw7_816hM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "axisx, axisy = [], []\n",
        "\n",
        "for x in range(0, 200):\n",
        "    x = x/100.\n",
        "    #logging.info(str(x))\n",
        "    acc = test_fusion(inference_result, softmax=False, weight = [1,0.23,1.5,0.3,0,0], verbose=False)\n",
        "    axisx.append(x)\n",
        "    axisy.append(acc)\n",
        "    \n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(axisx, axisy)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "i = np.argmax(np.array(axisy))\n",
        "print(axisx[i], axisy[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-5-IHCiFMM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_fusion(inference_result, softmax=True, weight = [1,0.9,0,0,0], verbose=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}